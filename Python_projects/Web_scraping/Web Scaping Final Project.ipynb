{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping GitHub using Beautiful Soup \n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll extract a list of each topic and for each topic we'll have a Topic title, Topic title URL, description of the topic\n",
    "- Each topic will  have 30 repositories\n",
    "- Each repository will have Repository name, username, stars and URL\n",
    "- DIfferent CSV files for different topics\n",
    "\n",
    "#### NOTE: The detailed construction of functions and its working with examples can be studied in the rough project available separately "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing the required libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Defining the Web URL we intend to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_url='https://github.com/topics'\n",
    "base_url=\"https://github.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Defining functions for extracting the list of topics from URL\n",
    "\n",
    "#### Following information parameters are suppose to be extracted\n",
    "- Topic Title\n",
    "- Topic Description\n",
    "- URL of the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A FUNCTION TO PARSE TOPIC TITLES\n",
    "def topic_titles(doc):  \n",
    "    topic_selection_class=\"f3 lh-condensed mb-0 mt-1 Link--primary\"  #create a variable containing the class of which the topic title is a part of\n",
    "    topic_title_tags= doc.find_all('p',{'class':topic_selection_class})  #finding all the 'p' tags with the defined class gives us list of names with the class \n",
    "    topic_titles=[]\n",
    "    for tags in topic_title_tags:\n",
    "        topic_titles.append(tags.text)\n",
    "    return topic_titles\n",
    "\n",
    "\n",
    "# CREATING A FUNCTION TO PARSE TOPIC DESCRIPTION\n",
    "def topic_descr(doc):  #uses same procedure as topic title\n",
    "    description_selection_class=\"f5 color-fg-muted mb-0 mt-1\"\n",
    "    topic_description_tags=doc.find_all('p', {'class': description_selection_class})\n",
    "    topic_desc=[]\n",
    "    for tags in topic_description_tags:\n",
    "        topic_desc.append(tags.text.strip())\n",
    "    return topic_desc\n",
    "\n",
    "\n",
    "# CREATING A FUNCTION TO PARSE TOPIC URL\n",
    "def topic_link(doc):  #uses same procedure as topic title\n",
    "    link_class= \"no-underline flex-grow-0\"\n",
    "    topic_link_tags=doc.find_all('a',{'class':link_class})\n",
    "    topic_links=[]\n",
    "    for tags in topic_link_tags:\n",
    "        topic_links.append('https://github.com'+ tags.get('href'))\n",
    "    return topic_links\n",
    "\n",
    "\n",
    "# COMBINING FUNCTIONS TO CREATE A DATA SET\n",
    "def scrape_topics():\n",
    "    \n",
    "    topics_url= 'https://github.com/topics'\n",
    "    response= requests.get(topics_url)\n",
    "    if response.status_code!=200:\n",
    "        raise Exception('Failed to load page {}'.format(topics_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #We make a dictionary of the extracted parameters using the predefined funcions. \n",
    "    #It is an easy method to transform multiple lists into a data frame using pandas\n",
    "    \n",
    "    topic_dictionary={\n",
    "        \"Topic title\": topic_titles(doc),\n",
    "        \"Topic description\": topic_descr(doc),\n",
    "        \"Topic link\": topic_link(doc)\n",
    "    }\n",
    "    \n",
    "    maintopic_df=pd.DataFrame(topic_dictionary)  #Transforming the dictionary to a data frame\n",
    "    maintopic_df.to_csv(\"Topics List\", index=None)  #Saving a CSV file of the obtained data frame\n",
    "    return maintopic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Defining the functions for extracting repository data from the topics\n",
    "\n",
    "\n",
    "#### Following information parameters are suppose to be extracted\n",
    "- Repository Title\n",
    "- Repository Owner User-name\n",
    "- URL of the Repository\n",
    "- Stars obtained by the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENDING AN HTTP REQUEST USING THE REQUESTS MODULE\n",
    "def get_topic_doc(topic_url):\n",
    "    response= requests.get(topic_url)\n",
    "    if response.status_code!=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    topic_doc=BeautifulSoup(response.text, 'html.parser')\n",
    "    return (topic_doc)\n",
    "\n",
    "\n",
    "# FUNCTION TO CONVERT THE STAR COUNT INTO A CALCULABEL NUMBER\n",
    "def parse_star_count(stars_str):\n",
    "    stars_str=stars_str.strip()\n",
    "    if stars_str[-1]== \"k\":\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)\n",
    "\n",
    "\n",
    "# DEFINING FUNCTION TO EXTRACT REPOSITORY PARAMETERS\n",
    "def give_repo_info(h1_tag, star_tag):\n",
    "    a_tags=h1_tag.find_all('a')   \n",
    "    #Both the name and username are located in the same tag\n",
    "    #Therefore we divide the tag and obtain both parameters \n",
    "    username=a_tags[0].text.strip()\n",
    "    repo_name=a_tags[1].text.strip()\n",
    "    repo_url=base_url+ a_tags[1].get('href')\n",
    "    stars=parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, repo_url, stars\n",
    "\n",
    "\n",
    "# COMBINING FUNCTIONS TO CREATE A DATASET FOR REPOSITORIES\n",
    "def get_topic_repos(topic_doc):\n",
    "    #Defining parameters necessary to be used in previous function \n",
    "    h3_class= \"f3 color-fg-muted text-normal lh-condensed\"\n",
    "    repo_tags= topic_doc.find_all('h3',{'class': h3_class})\n",
    "    \n",
    "    star_class=\"Counter js-social-count\"\n",
    "    star_tags=topic_doc.find_all('span',{'class':star_class})\n",
    "    \n",
    "    repo_dict={\"Username\":[], \"Repository Name\":[], \"Repository Link\":[], \"Stars\":[]}  #Making a dictionary to create data frames for repositories\n",
    "    \n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info=give_repo_info(repo_tags[i], star_tags[i])\n",
    "        repo_dict[\"Username\"].append(repo_info[0])\n",
    "        repo_dict[\"Repository Name\"].append(repo_info[1])\n",
    "        repo_dict[\"Repository Link\"].append(repo_info[2])\n",
    "        repo_dict[\"Stars\"].append(repo_info[3])\n",
    "    \n",
    "    return pd.DataFrame(repo_dict)\n",
    "\n",
    "\n",
    "# NAMING CONVENTION FOR REPOSITORY FILES\n",
    "def scrape_topic(topic_url, topic_name):\n",
    "    fname=topic_name + '.csv'\n",
    "    if os.path.exists(fname):\n",
    "        print('This one already exits bro!! Imma skip {}'.format(fname))  #Adding a condtion in case the dataset is already extracted \n",
    "        return\n",
    "    topic_df= get_topic_repos(get_topic_doc(topic_url))  #Combining topic functions\n",
    "    topic_df.to_csv(fname, index=None)  #Saving CSV files for repository parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Summing up both the procedures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df= scrape_topics()\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for {}'.format(row['Topic title']))\n",
    "        scrape_topic(row['Topic link'], row['Topic title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE OUTPUT...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping list of topics\n",
      "scraping top repositories for 3D\n",
      "scraping top repositories for Ajax\n",
      "scraping top repositories for Algorithm\n",
      "scraping top repositories for Amp\n",
      "scraping top repositories for Android\n",
      "scraping top repositories for Angular\n",
      "scraping top repositories for Ansible\n",
      "scraping top repositories for API\n",
      "scraping top repositories for Arduino\n",
      "scraping top repositories for ASP.NET\n",
      "scraping top repositories for Atom\n",
      "scraping top repositories for Awesome Lists\n",
      "scraping top repositories for Amazon Web Services\n",
      "scraping top repositories for Azure\n",
      "scraping top repositories for Babel\n",
      "scraping top repositories for Bash\n",
      "scraping top repositories for Bitcoin\n",
      "scraping top repositories for Bootstrap\n",
      "scraping top repositories for Bot\n",
      "scraping top repositories for C\n",
      "scraping top repositories for Chrome\n",
      "scraping top repositories for Chrome extension\n",
      "scraping top repositories for Command line interface\n",
      "scraping top repositories for Clojure\n",
      "scraping top repositories for Code quality\n",
      "scraping top repositories for Code review\n",
      "scraping top repositories for Compiler\n",
      "scraping top repositories for Continuous integration\n",
      "scraping top repositories for COVID-19\n",
      "scraping top repositories for C++\n"
     ]
    }
   ],
   "source": [
    "scrape_topic_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories for 3D\n",
      "This one already exits bro!! Imma skip 3D.csv\n",
      "Scraping top repositories for Ajax\n",
      "This one already exits bro!! Imma skip Ajax.csv\n",
      "Scraping top repositories for Algorithm\n",
      "This one already exits bro!! Imma skip Algorithm.csv\n",
      "Scraping top repositories for Amp\n",
      "This one already exits bro!! Imma skip Amp.csv\n",
      "Scraping top repositories for Android\n",
      "This one already exits bro!! Imma skip Android.csv\n",
      "Scraping top repositories for Angular\n",
      "This one already exits bro!! Imma skip Angular.csv\n",
      "Scraping top repositories for Ansible\n",
      "This one already exits bro!! Imma skip Ansible.csv\n",
      "Scraping top repositories for API\n",
      "This one already exits bro!! Imma skip API.csv\n",
      "Scraping top repositories for Arduino\n",
      "This one already exits bro!! Imma skip Arduino.csv\n",
      "Scraping top repositories for ASP.NET\n",
      "This one already exits bro!! Imma skip ASP.NET.csv\n",
      "Scraping top repositories for Atom\n",
      "This one already exits bro!! Imma skip Atom.csv\n",
      "Scraping top repositories for Awesome Lists\n",
      "This one already exits bro!! Imma skip Awesome Lists.csv\n",
      "Scraping top repositories for Amazon Web Services\n",
      "This one already exits bro!! Imma skip Amazon Web Services.csv\n",
      "Scraping top repositories for Azure\n",
      "This one already exits bro!! Imma skip Azure.csv\n",
      "Scraping top repositories for Babel\n",
      "This one already exits bro!! Imma skip Babel.csv\n",
      "Scraping top repositories for Bash\n",
      "This one already exits bro!! Imma skip Bash.csv\n",
      "Scraping top repositories for Bitcoin\n",
      "This one already exits bro!! Imma skip Bitcoin.csv\n",
      "Scraping top repositories for Bootstrap\n",
      "This one already exits bro!! Imma skip Bootstrap.csv\n",
      "Scraping top repositories for Bot\n",
      "This one already exits bro!! Imma skip Bot.csv\n",
      "Scraping top repositories for C\n",
      "This one already exits bro!! Imma skip C.csv\n",
      "Scraping top repositories for Chrome\n",
      "This one already exits bro!! Imma skip Chrome.csv\n",
      "Scraping top repositories for Chrome extension\n",
      "This one already exits bro!! Imma skip Chrome extension.csv\n",
      "Scraping top repositories for Command line interface\n",
      "This one already exits bro!! Imma skip Command line interface.csv\n",
      "Scraping top repositories for Clojure\n",
      "This one already exits bro!! Imma skip Clojure.csv\n",
      "Scraping top repositories for Code quality\n",
      "This one already exits bro!! Imma skip Code quality.csv\n",
      "Scraping top repositories for Code review\n",
      "This one already exits bro!! Imma skip Code review.csv\n",
      "Scraping top repositories for Compiler\n",
      "This one already exits bro!! Imma skip Compiler.csv\n",
      "Scraping top repositories for Continuous integration\n",
      "This one already exits bro!! Imma skip Continuous integration.csv\n",
      "Scraping top repositories for COVID-19\n",
      "This one already exits bro!! Imma skip COVID-19.csv\n",
      "Scraping top repositories for C++\n",
      "This one already exits bro!! Imma skip C++.csv\n"
     ]
    }
   ],
   "source": [
    "scrape_topic_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
